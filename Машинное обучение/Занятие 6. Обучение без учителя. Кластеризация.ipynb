{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Обучение без учителя преследует за собой две основные задачи:\n",
    "\n",
    "- сжатие размерности\n",
    "\n",
    "- кластеризация"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Кластеризация"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## k-means\n",
    "\n",
    "Метод k-means – это один из наиболее популярных методов кластеризации. Основная идея метода заключается в том, что на каждой итерации пересчитывается центр масс (центроид) для каждого кластера, полученного на предыдущем шаге, затем объекты снова разбиваются на кластеры согласно тому, какой из новых центроидов находится ближе.\n",
    "\n",
    "Более формально, алгоритм принимает на вход выборку $X_1, \\dots, X_N$ и параметр $k$, указывающий необходимое число кластеров. Выходом алгоритма является набор из $k$ центроидов $\\{\\mu_1, \\dots, \\mu_k\\}$, с помощью которых кластеризация осуществляется путём отнесения каждого объекту к ближайшему центроиду. Все точки внутри одного кластера ближе к центроиду этого кластера, чем к центроиду любого другого кластера.\n",
    "\n",
    "Метод может быть сформулирован как задача оптимизации, а именно, минимизации суммарного квадратичного отклонения точек кластеров от центров этих кластеров по центроидам и кластерам:$$\\sum_{i=1}^k \\sum_{X_n \\in C_i} ||X_n - \\mu_i||^2 \\rightarrow \\min, \\text{где $C_i$ - это $i$-ый кластер, $\\mu_i$ - это центр масс кластера $C_i$.}$$\n",
    "\n",
    "Решение такой задачи оптимизации является NP-трудной задачей, однако существует простой итеративный алгоритм, позволяющий найти локальный минимум указанного функционала. Алгоритм представляет собой последовательное чередование двух шагов до сходимости.\n",
    "\n",
    "**Алгоритм k-means**\n",
    "\n",
    "1. Выбрать количество кластеров, которое нам кажется оптимальным для наших данных.\n",
    "\n",
    "2. Выбрать случайным образом центроиды.\n",
    "\n",
    "3. Для каждой точки нашего набора данных посчитать, к какому центроиду она ближе.\n",
    "\n",
    "4. Переместить каждый центроид в центр той части выборки, которая отнесена к этому центроиду.\n",
    "\n",
    "5. Повторять последние два шага фиксированное число раз, либо до тех пор, пока центроиды не \"сойдутся\"\n",
    "(обычно это значит, что их смещение относительно предыдущего положения\n",
    "не превышает какого-то заранее заданного небольшого значения).\n",
    "\n",
    "Алгоритм гарантированно сходится,\n",
    "однако не гарантируется достижение глобального минимума – а только одного из локальных минимумов.\n",
    "\n",
    "Главным недостатком алгоритма является то, что итоговая кластеризация зависит от выбора исходных центров кластеров.\n",
    "\n",
    "На практике алгоритм запускается несколько раз из различных начальных приближений, а полученные результаты некоторым образом усредняются.\n",
    "\n",
    "Стоит также отметить, что число кластеров необходимо знать заранее. Существуют различные эвристики, позволяющие выбирать в некотором смысле оптимальное число кластеров.\n",
    "\n",
    "![](data:image/png;base64,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)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Кластеризация игроков NBA\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "nba = pd.read_csv(\"/Users/iakubovskii/Machine_Learning/RANEPA/Fintech_2020/Машинное обучение/Данные/nba_2013.csv\")\n",
    "nba.head(3)\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import DBSCAN\n",
    "from collections import Counter\n",
    "kmeans = KMeans(n_clusters=5, random_state=1)\n",
    "dbscan = DBSCAN(eps=0.5, min_samples=2)\n",
    "\n",
    "numeric_cols = nba._get_numeric_data().dropna(axis=1)\n",
    "kmeans.fit(numeric_cols)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "numeric_cols_sc = sc.fit_transform(numeric_cols)\n",
    "\n",
    "dbscan.fit(numeric_cols_sc)\n",
    "\n",
    "print(f\"Распределение кластеров k-means = {Counter(kmeans.labels_)}\",\n",
    "      \"\\n\",\n",
    "      f\"Распределение кластеров DBSCAN = {Counter(dbscan.labels_)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Визуализируем при помощи PCA\n",
    "pca = PCA(n_components=2)\n",
    "res = pca.fit_transform(numeric_cols)\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.scatter(res[:,0], res[:,1], c=kmeans.labels_, s=50, cmap='viridis')\n",
    "plt.title('PCA')\n",
    "\n",
    "# Используем 2 признака: Total points и Total assists\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.scatter(nba['pts'], nba['ast'], c=kmeans.labels_, s=50, cmap='viridis')\n",
    "plt.xlabel('Total points')\n",
    "plt.ylabel('Total assitances')\n",
    "plt.title('kmean')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Инициализация центроидов**\n",
    "\n",
    "*Метод sklearn.KMeans* содержит параметры ***n_init*** (число запусков из различных начальных приближений) и ***init***. Есть три способа инициализации центроидов:\n",
    "\n",
    "- k-means++ – \"умная\" инициализация центроидов для ускорения сходимости.\n",
    "\n",
    "- random – случайная инициализация центроидов.\n",
    "\n",
    "- ndarray – заданная инициализация центроидов."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Кластеризация рукописных цифр набора MNIST\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "digits = load_digits()\n",
    "\n",
    "X, y = digits.data, digits.target\n",
    "print(X.shape)\n",
    "\n",
    "# f, axes = plt.subplots(5, 2, sharey=True, figsize=(16,6))\n",
    "plt.figure(figsize=(16, 6))\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    plt.imshow(X[i,:].reshape([8,8]));"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Разделим цифры при помощи двумерных признаков\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_reduced = pca.fit_transform(X)\n",
    "\n",
    "print('Projecting %d-dimensional data to 2D' % X.shape[1])\n",
    "\n",
    "plt.figure(figsize=(12,10))\n",
    "plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=y,\n",
    "            edgecolor='none', alpha=0.7, s=40,\n",
    "            cmap=plt.cm.get_cmap('nipy_spectral', 10))\n",
    "plt.colorbar()\n",
    "plt.title('MNIST. PCA projection')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Иерархическая кластеризация\n",
    "\n",
    "Идея иерархической кластеризации заключается в том, что на первом шаге у нас\n",
    "количество кластеров = количеству точек. Далее мы последовательно находим\n",
    "ближайшие друг к другу точки и в конце алгоритма получаем один целый кластер,\n",
    "состоящий из всех точек.\n",
    "Иерархическая кластеризация бывает двух видов - аггломеративная и дивизиональная.\n",
    "Рассмотрим здесь алгоритм аггломеративной кластеризации.\n",
    "\n",
    "1. Каждая точка - отдельный кластер.\n",
    "\n",
    "2. Формируем кластеры, которые состоят из двух ближайших друг к другу точек.\n",
    "\n",
    "3. Повторяем то же самое для кластеров из шага 2 до момента пока не\n",
    "останется 1 большой кластер.\n",
    "\n",
    "4. В конце получаем дендрограмму, которая позволяет нам выделять уже какое-то\n",
    "определенное количество кластеров.\n",
    "\n",
    "Существует несколько способов измерять расстояние между кластерами:\n",
    "\n",
    "- ближайшее расстояние\n",
    "\n",
    "- наибольшее расстояние\n",
    "\n",
    "- расстояние между центроидами двух кластеров"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X = np.array([[5,3],\n",
    "    [10,15],\n",
    "    [15,12],\n",
    "    [24,10],\n",
    "    [30,30],\n",
    "    [85,70],\n",
    "    [71,80],\n",
    "    [60,78],\n",
    "    [70,55],\n",
    "    [80,91],])\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "labels = range(1, 11)\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.subplots_adjust(bottom=0.1)\n",
    "plt.scatter(X[:,0],X[:,1], label='True Position')\n",
    "\n",
    "for label, x, y in zip(labels, X[:, 0], X[:, 1]):\n",
    "    plt.annotate(\n",
    "        label,\n",
    "        xy=(x, y), xytext=(-3, 3),\n",
    "        textcoords='offset points', ha='right', va='bottom')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "linked = linkage(X, 'single')\n",
    "\n",
    "labelList = range(1, 11)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "dendrogram(linked,\n",
    "            orientation='top',\n",
    "            labels=labelList,\n",
    "            distance_sort='descending',\n",
    "            show_leaf_counts=True)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Видим, что в данном случае ярко-выражены 2 кластера."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "cluster = AgglomerativeClustering(n_clusters=2, affinity='euclidean', linkage='ward')\n",
    "cluster.fit_predict(X)\n",
    "\n",
    "plt.scatter(X[:,0],X[:,1], c=cluster.labels_, cmap='rainbow')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Метрики кластеризации\n",
    "\n",
    "Задача оценки качества кластеризации является более сложной по сравнению с оценкой качества классификации. Во-первых, такие оценки не должны зависеть от самих значений меток, а только от самого разбиения выборки. Во-вторых, не всегда известны истинные метки объектов, поэтому также нужны оценки, позволяющие оценить качество кластеризации, используя только неразмеченную выборку.\n",
    "\n",
    "Выделяют внешние и внутренние метрики качества. Внешние используют информацию об истинном разбиении на кластеры, в то время как внутренние метрики не используют никакой внешней информации и оценивают качество кластеризации, основываясь только на наборе данных. Оптимальное число кластеров обычно определяют с использованием внутренних метрик.\n",
    "\n",
    "Все указанные ниже метрики реализованы в sklearn."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Adjusted Rand Index (ARI)\n",
    "\n",
    "Предполагается, что известны истинные метки объектов. Данная мера не зависит от самих значений меток, а только от разбиения выборки на кластеры. Пусть $N$ – число объектов в выборке. Обозначим через $a$ число пар объектов, имеющих одинаковые метки и находящихся в одном кластере, через $b$ – число пар объектов, имеющих различные метки и находящихся в разных кластерах. Тогда Rand Index это$$\\text{RI} = \\frac{2(a + b)}{N(N-1)}.$$То есть это доля объектов, для которых эти разбиения (исходное и полученное в результате кластеризации) \"согласованы\". Rand Index (RI) выражает схожесть двух разных кластеризаций одной и той же выборки. Чтобы этот индекс давал значения близкие к нулю для случайных кластеризаций при любом $N$ и числе кластеров, необходимо нормировать его. Так определяется Adjusted Rand Index:$$\\text{ARI} = \\frac{\\text{RI} - E[\\text{RI}]}{\\max(\\text{RI}) - E[\\text{RI}]}.$$\n",
    "\n",
    "Эта мера симметрична, не зависит от значений и перестановок меток. Таким образом, данный индекс является мерой расстояния между различными разбиениями выборки. $\\text{ARI}$ принимает значения в диапазоне $[-1, 1]$. Значения, близкие к нулю соответствуют случайным разбиениям, а положительные значения говорят о том, что два разбиения схожи (совпадают при $\\text{ARI} = 1$).\n",
    "\n",
    "## Adjusted Mutual Information (AMI)\n",
    "\n",
    "Данная мера очень похожа на $\\text{ARI}$. Она также симетрична, не зависит от значений и перестановок меток. Определяется с использованием функции энтропии, интерпретируя разбиения выборки, как дискретные распределения (вероятность отнесения к кластеру равна доле объектов в нём). Индекс $MI$ определяется как взаимная информация для двух распределений, соответствующих разбиениям выборки на кластеры. Интуитивно, взаимная информация измеряет долю информации, общей для обоих разбиений: насколько информация об одном из них уменьшает неопределенность относительно другого.\n",
    "\n",
    "Аналогично $\\text{ARI}$ определяется индекс $\\text{AMI}$, позволяющий избавиться от роста индекса $MI$ с увеличением числа классов. Он принимает значения в диапазоне $[0, 1]$. Значения, близкие к нулю, говорят о независимости разбиений, а близкие к единице – об их схожести (совпадении при $\\text{AMI} = 1$).\n",
    "\n",
    "## Homogeneity, completeness, V-measure\n",
    "\n",
    "Формально данные меры также определяются с использованием функций энтропии и условной энтропии, рассматривая разбиения выборки как дискретные распределения:$$h = 1 - \\frac{H(C\\mid K)}{H(C)}, c = 1 - \\frac{H(K\\mid C)}{H(K)},$$здесь $K$ – результат кластеризации, $C$ – истинное разбиение выборки на классы. Таким образом, $h$ измеряет, насколько каждый кластер состоит из объектов одного класса, а $c$ – насколько объекты одного класса относятся к одному кластеру. Эти меры не являются симметричными. Обе величины принимают значения в диапазоне $[0, 1]$, и большие значения соответствуют более точной кластеризации. Эти меры не являются нормализованными, как $\\text{ARI}$ или $\\text{AMI}$, и поэтому зависят от числа кластеров. Случайная кластеризация не будет давать нулевые показатели при большом числе классов и малом числе объектов. В этих случаях предпочтительнее использовать $\\text{ARI}$. Однако при числе объектов более 1000 и числе кластеров менее 10 данная проблема не так явно выражена и может быть проигнорирована.\n",
    "\n",
    "Для учёта обеих величин $h$ и $c$ одновременно вводится $V$-мера как их среднее гармоническое:$$v = 2\\frac{hc}{h+c}.$$Она является симметричной и показывает, насколько две кластеризации схожи между собой.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Выбор оптимального количества кластеров\n",
    "\n",
    "## Метод локтя\n",
    "\n",
    "При кластеризации методом k-средних количество кластеров чаще всего оценивают с помощью «метода локтя».\n",
    "Он подразумевает многократное циклическое исполнение алгоритма с увеличением количества выбираемых кластеров,\n",
    "а также последующим откладыванием на графике балла кластеризации, вычисленного как функция от количества кластеров.\n",
    "\n",
    "\n",
    "## Силуэт\n",
    "\n",
    "В отличие от описанных выше метрик, силуэт не предполагает знания истинных меток объектов и\n",
    "позволяет оценить качество кластеризации, используя только саму (неразмеченную) выборку и результат кластеризации.\n",
    "Сначала силуэт определяется отдельно для каждого объекта. Обозначим через $a$ – среднее расстояние от данного объекта\n",
    "до объектов из того же кластера, через $b$ – среднее расстояние от данного объекта до объектов из ближайшего кластера\n",
    "(отличного от того, в котором лежит сам объект). Тогда силуэтом данного объекта называется величина:\n",
    "$$s = \\frac{b - a}{\\max(a, b)}.$$\n",
    "Силуэтом выборки называется средняя величина силуэта объектов данной выборки. Таким образом, силуэт показывает,\n",
    "на сколько среднее расстояние до объектов своего кластера отличается от среднего расстояния до объектов других кластеров.\n",
    "Данная величина лежит в диапазоне $[-1, 1]$. Значения, близкие к -1, соответствуют плохим (разрозненным) кластеризациям,\n",
    "значения, близкие к нулю, говорят о том, что кластеры пересекаются и накладываются друг на друга, значения,\n",
    "близкие к 1, соответствуют \"плотным\", четко выделенным кластерам. Таким образом, чем больше силуэт,\n",
    "тем более четко выделены кластеры, и они представляют собой компактные, плотно сгруппированные облака точек.\n",
    "\n",
    "С помощью силуэта можно выбирать оптимальное число кластеров $k$ (если оно заранее не известно) –\n",
    "выбирается число кластеров, максимизирующее значение силуэта.\n",
    "В отличие от предыдущих метрик, силуэт зависит от формы кластеров и достигает больших значений на более\n",
    "выпуклых кластерах, получаемых с помощью алгоритмов, основанных на восстановлении плотности распределения.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "inertia = []\n",
    "for k in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=1).fit(X)\n",
    "    inertia.append(np.sqrt(kmeans.inertia_))\n",
    "\n",
    "plt.plot(range(1,11), inertia, marker='s');\n",
    "plt.xlabel('$k$')\n",
    "plt.ylabel('$J(C_k)$');\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import (adjusted_rand_score,\n",
    "                             adjusted_mutual_info_score,\n",
    "                             homogeneity_score,\n",
    "                             completeness_score,\n",
    "                             v_measure_score,\n",
    "                             silhouette_score)\n",
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "\n",
    "data = datasets.load_digits()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "X_scal = sc.fit_transform(X)\n",
    "\n",
    "algorithms = []\n",
    "algorithms.append(KMeans(n_clusters=10, random_state=17))\n",
    "algorithms.append(AgglomerativeClustering(n_clusters=10))\n",
    "\n",
    "data = []\n",
    "for algo in algorithms:\n",
    "    algo.fit(X_scal)\n",
    "    data.append(({\n",
    "        'ARI': adjusted_rand_score(y, algo.labels_),\n",
    "        'AMI': adjusted_mutual_info_score(y, algo.labels_),\n",
    "        'Homogenity': homogeneity_score(y, algo.labels_),\n",
    "        'Completeness': completeness_score(y, algo.labels_),\n",
    "        'V-measure': v_measure_score(y, algo.labels_),\n",
    "        'Silhouette': silhouette_score(X, algo.labels_)}))\n",
    "\n",
    "results = pd.DataFrame(data=data, columns=['ARI', 'AMI', 'Homogenity',\n",
    "                                           'Completeness', 'V-measure',\n",
    "                                           'Silhouette'],\n",
    "                       index=['K-means', 'Agglomerative'])\n",
    "\n",
    "results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## DBSCAN (Density-based spatial clustering of applications with noise)\n",
    "\n",
    "**Основная идея.**\n",
    "\n",
    "В отличие от KMeans мы не хотим заранее задавать количество кластеров, мы\n",
    "просто хотим найти кластера, которые являются некими сгустками данных,\n",
    "поэтому нам нужно задать всего лишь *два параметра*.\n",
    "\n",
    "Первый из них это *размер эпсилон окрестности* и *minPts*, что расшифровывается как минимальное количество\n",
    "точек в этой самой эпсилон окрестности. Тогда наши точки можно разделить на три типа.\n",
    "\n",
    "Первый тип точек - *core точки*, у которых в эпсилон окрестностей как минимум *minPts* соседей.\n",
    "Далее идут *border точки* - точки, которые не являются *core точками*,\n",
    "но являются достижимыми из *core точек*. Все остальные точки - это\n",
    "*noise точки*, у которых в эпсилон окрестности меньше, чем *minPts* соседей,\n",
    "и они недостижимы из других *core точек*.\n",
    "\n",
    "Ниже на картинке изображен пример разделения точек на 3 типа.\n",
    "\n",
    "![](https://www.researchgate.net/profile/Elias-Giacoumidis/publication/331586411/figure/fig1/AS:733955368173569@1552000004684/Example-of-DBSCAN-when-the-number-of-minimum-points-is-equal-to-4.ppm)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Описание алгоритма DBSCAN\n",
    "\n",
    "1. Находим точки в $\\epsilon$ окрестности каждой точки и выделяем *core точки* с более чем *minPts соседями*.\n",
    "\n",
    "2. Определяем *border-точки* - точки, у которых меньше *min Pts* соседей,\n",
    "но хотя бы один сосед - *core точка*.\n",
    "\n",
    "3. Определяем *noise-точки* - они не принадлежат ни к одной из точек из первых\n",
    "двух пунктов.\n",
    "\n",
    "4. Находим связные компоненты *core точек*, игнорируя все остальные точки.\n",
    "Последовательность связанных *core точек* через *border точки* будет являться кластером.\n",
    "\n",
    "5. Помечаем *border точки* тем цветом кластера, с которыми они граничат.\n",
    "В противном случае мы считаем точку *noise точкой*.\n",
    "\n",
    "![](https://www.pvsm.ru/images/2017/02/20/interesnye-algoritmy-klasterizacii-chast-vtoraya-DBSCAN-3.png)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Данный алгоритм работает хорошо на довольно зашумленных данных и позволяет\n",
    "выявить хитроумные кластеры. Например, вот такие.\n",
    "![](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQjkj-Bg7HzIHgHgHPaZU2CgF9G9G9HN68wsw&usqp=CAU)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Плюсы DBSCAN:**\n",
    "- Не нужно задавать заранее количество кластеров так, как это было в алгоритме KMeans,\n",
    "то есть DBSCAN может найти любое количество кластеров.\n",
    "\n",
    "- В отличие от KMeans, кластеры могут быть причудливой формы\n",
    "\n",
    "- Умеет работать с шумными данными, что позволяет выявлять кластеры,\n",
    "которые KMeans или PCA выявить не в состоянии.\n",
    "\n",
    "**Минусы:**\n",
    "\n",
    "- На каждом шаге нужно искать ближайших соседей, и в худшем случае это будет квадратичный алгоритм\n",
    "(у нас есть n точек и для каждой из них мы проверим все остальные n на то,\n",
    "что они являются к ней ближайшими. В конечном счете получается, что задача решается\n",
    "за $O(N^2)$: если n достаточно большое, например, миллион,\n",
    "то эта задача решается слишком долго и к большим данным этот алгоритм не\n",
    "рекомендуется к применению.\n",
    "\n",
    "Есть ещё одна модификация этого алгоритма, который называется\n",
    "иерархический DBSCAN. Это выходит за рамки нашего курса,\n",
    "ничто не мешает ознакомиться с ним вам самим.\n",
    "HDBSCAN считается одним из лучших алгоритмов кластеризации на сегодня.\n",
    "Однако, применимость его к большим данным тоже вызывает сомнения."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn.datasets import make_circles\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import DBSCAN\n",
    "X, y = make_circles(n_samples=750, factor=0.3, noise=0.1)\n",
    "X = StandardScaler().fit_transform(X)\n",
    "y_pred = DBSCAN(eps=0.3, min_samples=10).fit_predict(X)\n",
    "\n",
    "plt.scatter(X[:,0], X[:,1], c=y_pred);"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Визуализация многомерного пространства при помощи t-SNE\n",
    "\n",
    "Название метода – t-distributed Stohastic Neighbor Embedding. Основная идея довольно простая простая - нужно\n",
    "найти такое отображение из многомерного признакового пространства на плоскость (или в 3D, но почти всегда выбирают 2D),\n",
    "чтобы точки, которые были далеко друг от друга, на плоскости тоже оказались удаленными, а близкие точки –\n",
    "также отобразились на близкие. По сути, neighbor embedding – это своего рода поиск нового представления данных,\n",
    "при котором сохраняется соседство. В основе метода лежит метрика расстояний между точками. Соответственно, как и в случае\n",
    "метода kNN, перед применением t-SNE мы должны стандартизировать все признаки.\n",
    "Подробней с методом можете ознакомиться [тут](https://habr.com/ru/post/267041/)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Визуализация игроков NBA при помощи t-SNE.\n",
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(random_state=17)\n",
    "X, y = digits.data, digits.target\n",
    "X_tsne = tsne.fit_transform(X)\n",
    "\n",
    "plt.figure(figsize=(12,10))\n",
    "plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y,\n",
    "            edgecolor='none', alpha=0.7, s=40,\n",
    "            cmap=plt.cm.get_cmap('nipy_spectral', 10))\n",
    "plt.colorbar()\n",
    "plt.title('MNIST. t-SNE projection')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}