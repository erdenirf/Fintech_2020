{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Обучение без учителя преследует за собой две основные задачи:\n",
    "\n",
    "- сжатие размерности\n",
    "\n",
    "- кластеризация"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Сжатие размерности\n",
    "\n",
    "## PCA - метод главных компонент\n",
    "\n",
    "### Сингулярное разложение\n",
    "\n",
    "\n",
    "*Сингулярным разложением* матрицы $X$ называется представление её в виде $X = U\\Sigma V^T$, где:\n",
    "\n",
    " - $\\Sigma$ есть $m\\times n$ матрица у которой элементы, лежащие на главной диагонали, неотрицательны, а все остальные элементы равны нулю.\n",
    " - $U$ и $V$ – ортогональные матрицы порядка $m$  и $n$ соответственно.\n",
    "\n",
    "Напомним, что ортогональная матрица - это матрица, для которой выполняется: $$A A^T = A^T A = E$$\n",
    " \n",
    "Рассмотрим более подробно задачу о сингулярном разложении матрицы $X \\in \\mathbb{R}^{m \\times n}$.\n",
    " \n",
    "Элементы главной диагонали матрицы $D$ называются *сингулярными числами* матрицы $X$, а столбцы $U$ и $V$ левыми и правыми *сингулярными векторами* матрицы $X$.\n",
    "\n",
    "Заметим, что матрицы $XX^T$ и $X^TX$ являются симметрическими неотрицательно определенными матрицами, и поэтому ортогональным преобразованием могут быть приведены к диагональному виду, причем на диагонали будут стоять неотрицательные собственные значения этих матриц.\n",
    "\n",
    "В силу указанных выше свойств матриц $X^TX$ и $XX^T$ сингулярное разложение матрицы $X$ тесно связано с задачей о спектральном разложении этих матриц. Более точно:\n",
    "- Левые сингулярные векторы матрицы $X$ (составляют матрицу U)– это собственные векторы матрицы $XX^T$.\n",
    "- Правые сингулярные векторы матрицы $X$ (составляют матрицу V) – это собственные векторы матрицы $X^TX$.\n",
    "- Сингулярные числа матрицы $X$ - это корни из собственных значений матрицы $X^TX$ (или $XX^T$).\n",
    "\n",
    "Таким образом, для нахождения сингулярного разложения матрицы $X$ необходимо, найти собственные векторы и значения матриц $X^TX$ и $XX^T$\n",
    "и составить из них матрицы $U, V, \\Sigma$.\n",
    "\n",
    "![Графическая интерпретация сингулярного уравнения](https://miro.medium.com/max/700/1*6wkgGgBy2NLVmRVOw8K86w.png)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.linalg import svd\n",
    "# define a matrix\n",
    "X = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "print(X)\n",
    "# SVD\n",
    "U, s, VT = svd(X)\n",
    "print(U)\n",
    "print(s)\n",
    "print(VT)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Проверим написанные нами выше формулы\n",
    "w_left, v_left = np.linalg.eig(X @ X.T)\n",
    "print(v_left)  # - матрица U\n",
    "w_right, v_right = np.linalg.eig(X.T @ X)\n",
    "print(v_right.T)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Идея и реализация метода PCA\n",
    "\n",
    "Основная идея метода главных компонент заключается в нахождении таких попарно ортогональных направлений в исходном многомерном пространстве (признаковом), вдоль которых данные имеют наибольший разброс (выборочную дисперсию). Эти направления называются главными компонентами. \n",
    "\n",
    "Другая формулировка PCA – для данной многомерной случайной величины построить такое ортогональное преобразование координат, что в результате корреляции между отдельными координатами обратятся в ноль. Таким образом, задача сводится к диагонализации матрицы ковариаций, что эквивалентно нахождению сингулярного разложения матрицы исходных данных.\n",
    "\n",
    "**Алгоритм**:\n",
    "\n",
    "1. Определить $k<n$ – новую размерность (k - компонент, n - исходное количество признаков)\n",
    "2. Нормализовать данные:\n",
    " - Вычесть из $X$ среднее, то есть заменить все $\\Large x^{(i)}$  на $$\\Large  x^{(i)} - \\frac{1}{m} \\sum_{i=1}^{m}{x^{(i)}}$$\n",
    " - Привести данные к единичной дисперсии: посчитать $$\\Large  \\sigma_j^2 = \\frac{1}{m} \\sum_{i=1}^{m}{(x^{(i)})^2}$$\n",
    "и заменить $\\Large x_j^{(i)}$ на $\\Large \\frac{x_j^{(i)}}{\\sigma_j}$ \n",
    "3. Найти сингулярное разложение матрицы $X$:\n",
    "$$\\Large X = UDV^T$$\n",
    "4. Положить $V =$ [$k$ левых столбцов матрицы $V$]\n",
    "5. Вернуть новую матрицу $$\\Large Z = XV \\in \\mathbb{R}^{m \\times k}$$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Пример реализации"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "np.random.seed(0)\n",
    "mean = np.array([0.0, 0.0])\n",
    "cov = np.array([[1.0, -1.0], \n",
    "                [-2.0, 3.0]])\n",
    "X = np.random.multivariate_normal(mean, cov, 300)\n",
    "\n",
    "pca = PCA()\n",
    "X_scal = (X - X.mean(axis=0))/X.std(axis=0)\n",
    "pca.fit(X_scal)\n",
    "print('Proportion of variance explained by each component:\\n' +\\\n",
    "      '1st component - %.2f,\\n2nd component - %.2f\\n' %\n",
    "      tuple(pca.explained_variance_ratio_))\n",
    "print('Directions of principal components:\\n' +\\\n",
    "      '1st component:', pca.components_[0],\n",
    "      '\\n2nd component:', pca.components_[1])\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter(X[:, 0], X[:, 1], s=50, c='r')\n",
    "for l, v in zip(pca.explained_variance_ratio_, pca.components_):\n",
    "    d = 5 * np.sqrt(l) * v\n",
    "    plt.plot([0, d[0]], [0, d[1]], '-k', lw=3)\n",
    "plt.axis('equal')\n",
    "plt.title('2D normal distribution sample and its principal components')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Первая главная компонента (ей соответствует более длинный вектор) объясняет более 90% дисперсии исходных данных. Это говорит о том, что она содержит в себе почти всю информацию о расположении выборки в пространстве, и вторая компонента может быть опущена. Спроецируем данные на первую компоненту."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Реализация через SVD\n",
    "\n",
    "U, s, Vh = svd(X_scal, full_matrices=False)\n",
    "n_comp = 2\n",
    "Z1 = U[:, :n_comp] * s[:n_comp]\n",
    "Z2 = X_scal @ Vh.T\n",
    "Z1[:10]\n",
    "print(np.allclose(Z1, Z2))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "pca.fit(X_scal)\n",
    "pca.transform(X_scal)[:10]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Заметим, что знаки не совпадают. SVD страдает от проблемы, называемой \"неопределенностью знака\",\n",
    "которая означает, что знак компонент и преобразованные данные зависят от алгоритма и случайного состояния. Для решения\n",
    "этой проблемы мы можем раскладывать не по всем векторам, а по зараннее выбранным (например, как в PCA - выбрать\n",
    "зараннее известное количество компонент): $ X_k = \\sum_{i=1}^k \\sigma_i u_i v_i $ вместо\n",
    "$ X_n = \\sum_{i=1}^n \\sigma_i u_i v_i $. Этот метод называется усеченным SVD (truncated SVD).\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "svd_trunc = TruncatedSVD(n_components=1)\n",
    "svd_trunc.fit(X_scal)\n",
    "result = svd_trunc.transform(X_scal)\n",
    "print(result[:10])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Оставим столько компонент, сколько нужно для объяснения по крайней мере 90% вариации\n",
    "pca = PCA(0.90)\n",
    "X_reduced = pca.fit_transform(X)\n",
    "\n",
    "# Преобразуем сниженные данные в изначальное признаковое пространство\n",
    "X_new = pca.inverse_transform(X_reduced)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(X[:, 0], X[:, 1], 'or', alpha=0.3)\n",
    "plt.plot(X_new[:, 0], X_new[:, 1], 'or', alpha=0.8)\n",
    "plt.axis('equal')\n",
    "plt.title('Проекция всей выборки на первую компоненту')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Мы понизили размерность данных вдвое, при этом сохранив наиболее значимые черты.\n",
    "В этом заключается основной принцип понижения размерности – приблизить многомерный набор данных\n",
    "с помощью данных меньшей размерности, сохранив при этом как можно больше информации об исходных данных."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## PCR - Principal component regression (регрессия на главные компоненты)\n",
    "\n",
    "PCR обычно используют в ситуациях, когда число факторов велико, а число наблюдений мало. В этом случае число степеней\n",
    "свободы будет стремиться к нулю, что плохо скажется на результатах. Помимо этого, PCR хорошо заходит в моделях,\n",
    "где факторы сильно коррелируют друг с другом.\n",
    "\n",
    "Пусть $Z_m$ - векторы новой размерности $m$, которые представлены через линейную комбинацию исходных векторов:  \n",
    "$$Z_m = \\sum_{j=1}^p \\phi_{jm}X_j$$\n",
    "\n",
    "где $\\phi_{jm}$ - вес фактора $j$ в компоненту $m$ согласно методу PCA\n",
    "\n",
    "Тогда после вычисления МНК-оценок данных векторов на целевую переменную, коэффициенты для исходных факторов находятся по такой формуле:\n",
    "\n",
    "$$\\beta_j = \\sum_{m=1}^M \\theta_m \\phi_{jm}$$\n",
    "\n",
    "где $\\theta_m$ - МНК оценка компоненты $m$ в линейной регрессии.\n",
    "\n",
    "Алгоритм метода PCR:\n",
    "\n",
    "1. Находим $m$ главных компонент из признакового пространства размерности $p$\n",
    "\n",
    "2. Строим регрессию зависимой переменной на эти главные компоненты.\n",
    "\n",
    "3. Полученные коэффициенты матрично перемножаем с весами, с которыми каждая исходная переменная входит в компоненту.\n",
    "\n",
    "4. Интерпретируем результаты. Поскольку все переменные стандартизированные, веса коэффициентов можно сравнивать друг с другом.\n",
    "\n",
    "Количество компонент в данном случае - гиперпараметр, который мы настраиваем при помощи кросс-валидации."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import category_encoders as ce\n",
    "import statsmodels.api as sm\n",
    "\n",
    "\n",
    "credit = pd.read_csv(\"/Users/iakubovskii/Machine_Learning/RANEPA/Fintech_2020/Машинное обучение/Данные/Credit.csv\")\n",
    "credit.set_index(\"ID\", inplace=True)\n",
    "\n",
    "qual_cols = ['Gender', 'Student', 'Married', 'Ethnicity']\n",
    "credit_reg = credit.drop(qual_cols, axis=1).join(pd.get_dummies(credit[qual_cols], drop_first=True))\n",
    "X_reg, y_reg = credit_reg.drop(\"Balance\", axis=1), credit_reg['Balance']\n",
    "sc = StandardScaler()\n",
    "X_reg_scal, y_reg_scal = sc.fit_transform(X_reg), sc.fit_transform(y_reg.values.reshape(-1,1))\n",
    "n_pca=2\n",
    "pca = PCA(n_components=n_pca).fit(X_reg_scal)\n",
    "pcs = pca.fit_transform(X_reg_scal)\n",
    "X_PCR =  pcs.copy()\n",
    "V = pca.components_.T\n",
    "ols = sm.OLS(endog = y_reg_scal, exog = sm.add_constant(X_PCR)).fit()\n",
    "print(ols.summary())\n",
    "beta_Z = ols.params[1:]\n",
    "beta_X = V @ beta_Z\n",
    "coefs = pd.DataFrame(index = X_reg.columns, data = beta_X)\n",
    "coefs.columns = ['coefs_PCR']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "categorical_features = ['Gender', 'Student', 'Married', 'Ethnicity']\n",
    "numeric_features = list(set(credit.columns) - set(categorical_features) - set([\"Balance\"]))\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler(with_std=True, with_mean=True))])\n",
    "\n",
    "categorical_transformer = ce.OneHotEncoder()\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "X_preprocessed = preprocessor.fit_transform(credit.drop(\"Balance\", axis=1))\n",
    "y = (credit['Balance'] - credit['Balance'].mean()) / credit['Balance'].std\n",
    "\n",
    "n_pca=2\n",
    "pca = PCA(n_components=n_pca).fit(X_preprocessed)\n",
    "pcs = pca.fit_transform(X_preprocessed)\n",
    "X_PCR =  pcs.copy()\n",
    "V = pca.components_.T\n",
    "ols = sm.OLS(endog = y, exog = sm.add_constant(X_PCR)).fit()\n",
    "print(ols.summary(xname=['const', 'PC1', \"PC2\"]))\n",
    "beta_Z = ols.params[1:]\n",
    "beta_X = V @ beta_Z\n",
    "all_cols = preprocessor.transformers_[0][2].copy()  # extract num columns (0 step in pipeline)\n",
    "cat_col_names = preprocessor.transformers_[1][1].get_feature_names()  # extract category columns (1 step in pipeline)\n",
    "all_cols.extend(cat_col_names)\n",
    "coefs = pd.DataFrame(index = all_cols, data = beta_X)\n",
    "coefs.columns = ['coefs_PCR']\n",
    "print(coefs)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Почему коэффициенты отличаются? Ваша задача с этим разобраться. А мы пока посмотрим, как снижается MSE при увеличении\n",
    "количества компонент в регрессии, а также как изменяются сами коэффициенты."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_mse(n_pca=2):\n",
    "    pca = PCA(n_components=n_pca).fit(X_reg_scal)\n",
    "    pcs = pca.fit_transform(X_reg_scal)\n",
    "    X_PCR =  pcs.copy()\n",
    "    V = pca.components_.T\n",
    "    ols = sm.OLS(endog = y_reg_scal, exog = sm.add_constant(X_PCR)).fit()\n",
    "    return ols.mse_model\n",
    "plt.plot(np.arange(1, 11),list(map(get_mse, np.arange(1, 11))));"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_coefs(n_pca=2):\n",
    "    pca = PCA(n_components=n_pca).fit(X_reg_scal)\n",
    "    pcs = pca.fit_transform(X_reg_scal)\n",
    "    X_PCR =  pcs.copy()\n",
    "    V = pca.components_.T\n",
    "    ols = sm.OLS(endog = y_reg_scal, exog = sm.add_constant(X_PCR)).fit()\n",
    "    beta_Z = ols.params[1:]\n",
    "    beta_X = V @ beta_Z\n",
    "    coefs = pd.DataFrame(index = X_reg.columns, data = beta_X)\n",
    "    coefs.columns = ['coefs_PCR_' + str(n_pca)]\n",
    "    return coefs\n",
    "get_coefs(3)\n",
    "from functools import reduce\n",
    "dfs = [get_coefs(i) for i in range(1, 11)]\n",
    "df_final = reduce(lambda left,right: left.join(right), dfs)\n",
    "df_final"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## PLS - partial least squares (частный МНК)\n",
    "\n",
    "PCR предполагает регрессию зависимой переменной на компоненты, которые получены из задачи обучения без учителя.\n",
    "PLS, в свою очередь, определяет новые переменные, используя зависимость изначальных факторов с целевым показателем.\n",
    "PLS помогает найти те направления, которые помогают объяснить как целевой фактор, так и объясняющие переменные.\n",
    "\n",
    "Алгоритм PLS выглядит примерно следующим образом.\n",
    "\n",
    "1. Первая компонента $Z_1$ находится по формуле $Z_1 = \\sum_{j=1}^p corr_{j1}^2 X_j$. \n",
    "\n",
    "2. На втором шаге мы корректируем все факторы по $Z_1$ - строим регрессии каждой переменной на $Z_1$ и находим остатки. Как мы помним, остатки - это необъясненная доля дисперсии целевой переменной. В нашем случае это будет необъясненная доля дисперсии первой компонентой. \n",
    "\n",
    "3. Аналогично, как и на первом шаге, для каждой компоненты считаем веса, только вместо $X_1$ будут их остатки от объяснения предыдущих компонент.\n",
    "\n",
    "4. Оцениваем зависимую переменную на полученные компоненты."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# PLS\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "pls = PLSRegression(n_components=2)\n",
    "pls.fit(X_reg_scal, y_reg_scal)\n",
    "print(f\"PLS r-squared with 2 components: {pls.score(X_reg_scal, y_reg_scal):.3f}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "pca_2 = make_pipeline(PCA(n_components=2), LinearRegression())\n",
    "pca_2.fit(X_reg_scal, y_reg_scal)\n",
    "print(f\"PCR r-squared with 2 components {pca_2.score(X_reg_scal, y_reg_scal):.3f}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}